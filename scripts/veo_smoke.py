#!/usr/bin/env python3
import json
import os
import random
import subprocess
import sys
import time
import uuid
from datetime import datetime, timezone

import vertexai
from vertexai.preview.generative_models import GenerationConfig, GenerativeModel

# Initialize Vertex AI
vertexai.init(project=os.environ.get("GCP_PROJECT"), location="us-central1")


def check_veo_tokens_available():
    """
    Check if there are enough tokens available for a minimal smoke test.
    Uses the monitoring API to get real-time token usage.

    Returns:
        bool: True if enough tokens are available, False otherwise
    """
    try:
        # Get current token usage from monitoring API
        project_id = os.environ.get("GCP_PROJECT")
        if not project_id:
            print("‚ùå No project ID specified. Set GCP_PROJECT environment variable.")
            return False

        # Check if gcloud monitoring is available
        check_cmd = ["gcloud", "help", "monitoring"]
        check_result = subprocess.run(
            check_cmd, capture_output=True, text=True, check=False
        )

        if (
            check_result.returncode != 0
            or "ERROR: (gcloud.monitoring)" in check_result.stderr
        ):
            print(
                "‚ö†Ô∏è gcloud monitoring not available in this environment, skipping token check"
            )
            return True

        # First check if time-series command is available
        cmd = ["gcloud", "monitoring", "time-series", "--help"]

        help_result = subprocess.run(cmd, capture_output=True, text=True, check=False)

        if (
            help_result.returncode != 0
            or "Invalid choice: 'time-series'" in help_result.stderr
        ):
            print("‚ö†Ô∏è gcloud monitoring time-series not available, skipping token check")
            return True

        # Now try to get the token usage
        cmd = [
            "gcloud",
            "monitoring",
            "time-series",
            "list",
            f"--project={project_id}",
            '--filter=metric.type="aiplatform.googleapis.com/generative/tokens_in_use" AND metric.label."base_model"="veo-3.0-generate-001"',
            "--limit=1",
            "--format=value(point.value.int64Value)",
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, check=False)

        if result.returncode != 0:
            print(f"‚ö†Ô∏è Could not check token usage: {result.stderr}")
            # Continue with the test if we can't check tokens
            return True

        # Parse the token usage
        tokens_in_use = 0
        if result.stdout.strip():
            tokens_in_use = int(result.stdout.strip())

        # Get max tokens per minute from env or use default 60
        max_tokens_per_minute = int(os.environ.get("VEO_LIMIT_MPM", 60))
        tokens_needed = 10  # Minimal smoke test needs ~10 tokens

        tokens_available = max_tokens_per_minute - tokens_in_use

        if tokens_available < tokens_needed:
            print(
                f"‚ö†Ô∏è Veo tokens busy ({tokens_in_use}/{max_tokens_per_minute}, {tokens_available} available) ‚Äì skipping smoke test."
            )
            return False

        print(
            f"‚úÖ Token check passed: {tokens_available} tokens available ({tokens_in_use} in use)"
        )
        return True

    except Exception as e:
        print(f"‚ö†Ô∏è Error checking token usage: {e}")
        # Continue with the test if we can't check tokens
        return True


def generate_test_clip():
    """
    Generate a test video clip using Veo.
    Returns 0 on success, 1 on error, 2 on quota limit.
    """
    # Check if we have tokens available before attempting the test
    if not check_veo_tokens_available():
        print(
            "‚ÑπÔ∏è Skipping smoke test due to token limitations. Allowing deploy to proceed."
        )
        return 0  # Allow deploy to proceed

    # Ultra minimal prompt for smoke test
    prompt = "smoke-test"
    print(f"üìã Using minimal prompt: {prompt}")

    # Veo model generation with retry logic
    max_retries = 3
    retry_count = 0

    while retry_count <= max_retries:
        try:
            if retry_count == 0:
                print("üìã Loading Veo model...")
            else:
                print(
                    f"üìã Retry attempt {retry_count}/{max_retries} after quota limit..."
                )

            model = GenerativeModel("veo-3.0-generate-preview")

            print("üé¨ Generating minimal token request...")

            # Configure for absolute minimum token usage
            gen_config = GenerationConfig(
                temperature=0.4,
                top_p=1.0,
                top_k=32,
                candidate_count=1,
                max_output_tokens=2048,
                # Add any params that would reduce token usage
            )

            start_time = time.time()

            # Use the cheapest possible prompt and configuration
            # Setting up the absolute minimal request
            response = model.generate_content(
                f"Generate a 5-second video in 16:9 aspect ratio of {prompt}.",
                generation_config=gen_config,
            )

            # Extended timeout for Veo response
            timeout = 600

            if hasattr(response, "candidates") and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, "content") and candidate.content:
                    for part in candidate.content.parts:
                        if hasattr(part, "video") and part.video:
                            duration = time.time() - start_time
                            print(
                                f"‚úÖ Video generated successfully in {duration:.1f} seconds"
                            )

                            # Don't save the video to save disk space, just verify we got something
                            print(
                                f"üîç Verified video data received ({len(part.video.file_data)} bytes)"
                            )
                            return 0

            print("‚ùå Veo API returned success but no video content found")
            return 1

        except Exception as e:
            error_str = str(e).lower()

            # Handle quota limit errors with intelligent wait
            if (
                "quota" in error_str
                or "resource exhausted" in error_str
                or "limit" in error_str
            ):
                retry_count += 1

                if retry_count > max_retries:
                    print(f"‚ùå Veo quota exceeded or insufficient tokens: {e}")
                    print("üí° Please check your Veo API quota in Google Cloud Console")
                    print(
                        "üí° Consider requesting a quota increase: https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai"
                    )
                    print(
                        "üí° This is expected in CI during deployment - considering this a soft pass"
                    )
                    return 0  # Return success to allow deployment to proceed

                # Wait for the next minute window instead of exponential backoff
                current_second = datetime.now(timezone.utc).second
                sleep_time = (
                    60 - current_second + 1
                )  # Wait until the start of the next minute
                print(f"‚è±Ô∏è Waiting for next quota window: {sleep_time}s")
                time.sleep(sleep_time)
            elif "permission" in error_str or "unauthorized" in error_str:
                print(f"‚ùå Permission error: {e}")
                print("üí° Check that your service account has access to Veo API")
                sys.exit(1)
            else:
                print(f"‚ùå Unexpected error: {e}")
                sys.exit(1)

    return 1


if __name__ == "__main__":
    print("üé• Running Veo API minimal-token smoke test...")
    result = generate_test_clip()
    sys.exit(result)
